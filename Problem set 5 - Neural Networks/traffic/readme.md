I began by using a single 2D convolution layer, a max pooling layer, a dense layer of 128 with the ReLU activation function as a hidden layer with a dropout of 0.5 and a softmax output layer. This yielded an accuracy of 0.0540 wih a loss of 3.5. I next added another identical dense layer (with dropout) which resulted in an accuracy of 0.0542 (so no real difference). After this, I increased the number of nodes in each of the dense layers to 500 which massively increased the accuracy to 0.8309 with a loss of 0.6187 (although time per step increased to 4ms/step from 3ms/step). I next added a third dense layer of 500 to see whether the returns were diminishing - although the accuracy increase was smaller (up to 0.8907) this still produced a higher accuracy and reduced loss to 0.4116 while increasing time per step to 6ms/step.

Now that the accuracy was approaching 0.9, I decided to change the middle dense layer to 1000 nodes. This only increased the accuracy to 0.9003 so wasn't worth the additional training time (so was changed back). Instead, I changed the first layer to 1000. This again only marginally increased the accuracy so was reversed. At this point tensor flow decided to break - despite no part of the code being changed, the accuracy refused to go higher than 0.05. This was for the same settings as the high yield of 0.8907. After a while, it then snapped out of it but the old settings only achieved 0.8076.